{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_IDs, files, ppm, tranform):\n",
    "        self.filenames = files\n",
    "        self.list_IDs = list_IDs\n",
    "        self.ppm = ppm\n",
    "        self.transform = tranform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        cv_in = cv2.imread('../img/'+self.filenames[ID]+'.jpg',1) # color\n",
    "        pil_in = Image.fromarray(cv_in)\n",
    "        X = self.transform(pil_in)\n",
    "        y = self.ppm[ID]\n",
    "        return X,y\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1),nn.ReLU(inplace=True),nn.Conv2d(out_channels, out_channels, 3, padding=1),nn.ReLU(inplace=True))\n",
    "\n",
    "class LeUNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeUNet,self).__init__()\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear') #, align_corners=True)\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        self.conv_last = nn.Conv2d(64, 1, 1)\n",
    "        self.features = nn.Sequential(nn.Conv2d(1, 6, 5),nn.ReLU(inplace=True),nn.MaxPool2d(2),nn.Conv2d(6, 16, 5),nn.ReLU(inplace=True),nn.MaxPool2d(2),)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((5, 5))\n",
    "        self.estimator = nn.Sequential(nn.Linear(16*5*5,120),nn.ReLU(inplace=True),nn.Linear(120, 84),nn.ReLU(inplace=True),nn.Linear(84, 1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "        x = self.dconv_down4(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LeUNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9ff29f042fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#data = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_pm_best.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LeUNet' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(loader,model,criterion):\n",
    "    model.eval()\n",
    "    loss = []\n",
    "    pred = []\n",
    "    actual = []\n",
    "    #with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        y = y.float()\n",
    "        x_var = torch.autograd.Variable(x)\n",
    "        y_var = torch.autograd.Variable(y)\n",
    "        yhat = model(x_var)\n",
    "        loss = criterion(yhat.squeeze(),y_var)\n",
    "        loss.append(loss.data[0])\n",
    "        print(y)\n",
    "        print(yhat.squeeze())\n",
    "        #actual.append(y[0]) #fix: print\n",
    "        #pred.append(yhat.squeeze()[0]) #print\n",
    "        \n",
    "    return pred,actual,loss\n",
    "\n",
    "\n",
    "data = pd.read_csv('../final_data.csv')\n",
    "files = list(data['filename'])\n",
    "ppm = list(data['ppm'])\n",
    "ids = [i for i in range(len(files))]\n",
    "#data = None\n",
    "\n",
    "model = LeUNet()\n",
    "model = torch.nn.DataParallel(model) #.cuda()\n",
    "model.load_state_dict(torch.load(\"model_pm_best.pth\"))\n",
    "criterion = nn.MSELoss() #.cuda()\n",
    "dataset = Dataset(ids, files, ppm, transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor(),transforms.Normalize(mean=[0.5231, 0.5180, 0.5115],std=[0.2014, 0.2018, 0.2100]),])) # normalize\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "pred,actual,loss = evaluate(loader,model,criterion) # plot preds vs actual\n",
    "# eval bad vs good examples from pred, actual (index numbered 0,1,2,...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
