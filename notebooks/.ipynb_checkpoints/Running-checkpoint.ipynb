{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "#import os\n",
    "#import random\n",
    "#import time\n",
    "#import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.parallel\n",
    "import torch.optim\n",
    "#iimport torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "#import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_IDs, files, ppm, tranform):\n",
    "        self.filenames = files\n",
    "        self.list_IDs = list_IDs\n",
    "        self.ppm = ppm\n",
    "        self.transform = tranform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        cv_in = cv2.imread('/scratch/ab9738/pollution_img/data/'+self.filenames[ID]+'.jpg',1) # color\n",
    "        pil_in = Image.fromarray(cv_in)\n",
    "        X = self.transform(pil_in)\n",
    "        y = self.ppm[ID]\n",
    "        return X,y\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1),nn.ReLU(inplace=True),nn.Conv2d(out_channels, out_channels, 3, padding=1),nn.ReLU(inplace=True))\n",
    "\n",
    "class LeUNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeUNet,self).__init__()\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear') #, align_corners=True)\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        self.conv_last = nn.Conv2d(64, 1, 1)\n",
    "        self.features = nn.Sequential(nn.Conv2d(1, 6, 5),nn.ReLU(inplace=True),nn.MaxPool2d(2),nn.Conv2d(6, 16, 5),nn.ReLU(inplace=True),nn.MaxPool2d(2),)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((5, 5))\n",
    "        self.estimator = nn.Sequential(nn.Linear(16*5*5,120),nn.ReLU(inplace=True),nn.Linear(120, 84),nn.ReLU(inplace=True),nn.Linear(84, 1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "        x = self.dconv_down4(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),16*5*5)\n",
    "        x = self.estimator(x)\n",
    "        return x\n",
    "\n",
    "class LeUNetEns(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeUNetEns,self).__init__()\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear') #, align_corners=True)\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        self.conv_last = nn.Conv2d(64, 1, 1)\n",
    "        self.features = nn.Sequential(nn.Conv2d(1, 6, 5),nn.ReLU(inplace=True),nn.MaxPool2d(2),nn.Conv2d(6, 16, 5),nn.ReLU(inplace=True),nn.MaxPool2d(2),)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((5, 5))\n",
    "        # self.estimator = nn.Sequential(nn.Linear(16*5*5,120),nn.ReLU(inplace=True),nn.Linear(120, 84),nn.ReLU(inplace=True),nn.Linear(84, 1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "        x = self.dconv_down4(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),16*5*5)\n",
    "        # x = self.estimator(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNetUNet,self).__init__()\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear') #, align_corners=True)\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        self.conv_last = nn.Conv2d(64, 1, 1)\n",
    "        self.features = models.resnet50()\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((5, 5))\n",
    "        self.estimator = nn.Sequential(nn.Linear(1000,120),nn.ReLU(inplace=True),nn.Linear(120, 84),nn.ReLU(inplace=True),nn.Linear(84, 1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "        x = self.dconv_down4(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "        x = self.features(x)\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0),16*5*5)\n",
    "        x = self.estimator(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StandardNet(nn.Module):\n",
    "\n",
    "    def __init__(self, modelname='resnet101'):\n",
    "        super(StandardNet, self).__init__()\n",
    "        self.model = eval(\"models.\"+modelname+\"()\")\n",
    "        if(modelname=='inception_v3'):\n",
    "            self.model = eval(\"models.\"+modelname+\"(aux_logits=False)\")\n",
    "        self.estimator = nn.Sequential(nn.Linear(1000,120),nn.ReLU(inplace=True),nn.Linear(120,84),nn.ReLU(inplace=True),nn.Linear(84, 1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.estimator(x)\n",
    "        return x\n",
    "\n",
    "class EnsembleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleNet, self).__init__()\n",
    "        self.resnet50_pred = models.resnet50()\n",
    "        self.vgg16_pred = models.vgg16()\n",
    "        self.inceptionv3_pred = models.inception_v3(aux_logits=False)\n",
    "        self.estimator = nn.Sequential(nn.Linear(3000,120),nn.ReLU(inplace=True),nn.Linear(120,84),nn.ReLU(inplace=True),nn.Linear(84,1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.resnet50_pred(x)\n",
    "        b = self.vgg16_pred(x)\n",
    "        c = self.inceptionv3_pred(x)\n",
    "        x = torch.cat((a,b,c),1)\n",
    "        x = self.estimator(x)\n",
    "        return x\n",
    "\n",
    "class EnsembleLeUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleLeUNet, self).__init__()\n",
    "        self.resnet50_pred = models.resnet50()\n",
    "        self.vgg16_pred = models.vgg16()\n",
    "        self.inceptionv3_pred = models.inception_v3(aux_logits=False)\n",
    "        self.leunet_pred = LeUNetEns()\n",
    "        # self.leunet_pred = torch.nn.DataParallel(self.leunet_pred).cuda()\n",
    "        self.leunet_pred.load_state_dict(torch.load(\"model_haze_all.pth\", map_location='cpu'),strict=False)\n",
    "        self.estimator = nn.Sequential(nn.Linear(3400,120),nn.ReLU(inplace=True),nn.Linear(120,84),nn.ReLU(inplace=True),nn.Linear(84,1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.resnet50_pred(x)\n",
    "        b = self.vgg16_pred(x)\n",
    "        c = self.inceptionv3_pred(x)\n",
    "        d = self.leunet_pred(x)\n",
    "        x = torch.cat((a,b,c,d),1)\n",
    "        x = self.estimator(x)\n",
    "        return x\n",
    "\n",
    "class NewReLU(nn.Module):\n",
    "    \"\"\" Custom ReLU layer for baseline \"\"\"\n",
    "    def __init__(self, alpha, beta):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha*max(x,0) + min(0,x)*self.beta/x+1e-7\n",
    "        return x\n",
    "\n",
    "class EPAPLN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EPAPLN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32,3)\n",
    "        self.conv2 = nn.Conv2d(32,32,3)\n",
    "        self.maxpool = nn.MaxPool2d(3, 2)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(32,64,3)\n",
    "        self.conv4 = nn.Conv2d(64,64,3)\n",
    "        self.conv5 = nn.Conv2d(64,64,3)\n",
    "        self.avgpool = nn.AvgPool2d(3, 2)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.conv6 = nn.Conv2d(64,96,3)\n",
    "        self.conv7 = nn.Conv2d(96,96,3)\n",
    "        self.conv8 = nn.Conv2d(96,96,3)\n",
    "        self.conv9 = nn.Conv2d(96,96,3)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((2,2))\n",
    "        self.estimator = nn.Sequential(nn.Linear(96*2*2,120),nn.ReLU(inplace=True),nn.Linear(120, 84),nn.ReLU(inplace=True),nn.Linear(84, 1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.adaptivepool(x)\n",
    "        x = x.view(x.size(0),96*2*2)\n",
    "        x = self.estimator(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(train_loader,model,criterion,optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    epoch_samples = 0\n",
    "    for x, y in train_loader:\n",
    "        epoch_samples += x.size(0)\n",
    "        y = y.float()\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        y = y.cuda(non_blocking=True)\n",
    "\n",
    "        x_var = torch.autograd.Variable(x)\n",
    "        y_var = torch.autograd.Variable(y)\n",
    "\n",
    "        yhat = model(x_var)\n",
    "        loss = criterion(yhat.squeeze(),y_var)\n",
    "        total_loss += loss.data.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (total_loss/epoch_samples)\n",
    "\n",
    "\n",
    "def val(val_loader,model,criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    epoch_samples = 0\n",
    "    #with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        epoch_samples += x.size(0)\n",
    "        y = y.float()\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        y = y.cuda(non_blocking=True)\n",
    "\n",
    "        x_var = torch.autograd.Variable(x)\n",
    "        y_var = torch.autograd.Variable(y)\n",
    "\n",
    "        yhat = model(x_var)\n",
    "        loss = criterion(yhat.squeeze(),y_var)\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "    return (total_loss/epoch_samples)\n",
    "\n",
    "def freeze_weights_leunet(model):\n",
    "    model.module.dconv_down1[0].weight.requires_grad, model.module.dconv_down1[0].bias.requires_grad = False, False\n",
    "    model.module.dconv_down2[0].weight.requires_grad, model.module.dconv_down2[0].bias.requires_grad = False, False\n",
    "    model.module.dconv_down3[0].weight.requires_grad, model.module.dconv_down3[0].bias.requires_grad = False, False\n",
    "    model.module.dconv_down4[0].weight.requires_grad, model.module.dconv_down4[0].bias.requires_grad = False, False\n",
    "    model.module.dconv_up1[0].weight.requires_grad, model.module.dconv_up1[0].bias.requires_grad = False, False\n",
    "    model.module.dconv_up2[0].weight.requires_grad, model.module.dconv_up2[0].bias.requires_grad = False, False\n",
    "    model.module.dconv_up3[0].weight.requires_grad, model.module.dconv_up3[0].bias.requires_grad = False, False\n",
    "    model.module.dconv_down1[2].weight.requires_grad, model.module.dconv_down1[2].bias.requires_grad = False, False\n",
    "    model.module.dconv_down2[2].weight.requires_grad, model.module.dconv_down2[2].bias.requires_grad = False, False\n",
    "    model.module.dconv_down3[2].weight.requires_grad, model.module.dconv_down3[2].bias.requires_grad = False, False\n",
    "    model.module.dconv_down4[2].weight.requires_grad, model.module.dconv_down4[2].bias.requires_grad = False, False\n",
    "    model.module.dconv_up1[2].weight.requires_grad, model.module.dconv_up1[2].bias.requires_grad = False, False\n",
    "    model.module.dconv_up2[2].weight.requires_grad, model.module.dconv_up2[2].bias.requires_grad = False, False\n",
    "    model.module.dconv_up3[2].weight.requires_grad, model.module.dconv_up3[2].bias.requires_grad = False, False\n",
    "    model.module.conv_last.weight.requires_grad, model.module.conv_last.bias.requires_grad = False, False\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('../train_data.csv')\n",
    "    data_train = data.sample(frac=0.8,random_state=17)\n",
    "    data_val = data.loc[~data.index.isin(data_train.index)]\n",
    "    files_train = list(data_train['filename'])\n",
    "    files_val = list(data_val['filename'])\n",
    "    ppm_train = list(data_train['ppm'])\n",
    "    ppm_val = list(data_val['ppm'])\n",
    "    ids_train = [i for i in range(len(files_train))]\n",
    "    ids_val = [i for i in range(len(files_val))]\n",
    "    data = None\n",
    "    data_train = None\n",
    "    data_val = None\n",
    "#     model = LeUNet()\n",
    "#     model = ResNetUNet()\n",
    "#     model = torch.nn.DataParallel(model).cuda()\n",
    "#     model.load_state_dict(torch.load(\"model_haze_all.pth\"),strict=False) # on GPU\n",
    "#     model = StandardNet('resnet50').cuda()\n",
    "#     model = StandardNet('vgg16').cuda()\n",
    "    model = StandardNet('inception_v3').cuda()\n",
    "#     model = EPAPLN().cuda()\n",
    "#     model = EnsembleNet().cuda()  \n",
    "#     model = EnsembleLeUNet().cuda()  \n",
    "\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=1e-4)\n",
    "    # model = freeze_weights_leunet(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "    train_dataset = Dataset(ids_train, files_train, ppm_train, transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor(),transforms.Normalize(mean=[0.5231, 0.5180, 0.5115],std=[0.2014, 0.2018, 0.2100]),])) # normalize\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12)\n",
    "    val_dataset = Dataset(ids_val, files_val, ppm_val, transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor(),transforms.Normalize(mean=[0.5231, 0.5180, 0.5115],std=[0.2014, 0.2018, 0.2100]),]))\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=12)\n",
    "\n",
    "    best_loss = 1e5\n",
    "    for epoch in range(500):\n",
    "        train_loss = train(train_loader,model,criterion,optimizer)\n",
    "        val_loss = val(val_loader,model,criterion)\n",
    "        print('Epoch: %d, MSE train set: %.8f' % (epoch+1, train_loss))\n",
    "        print('Epoch: %d, MSE val set: %.8f\\n' % (epoch+1, val_loss))\n",
    "        if val_loss < best_loss:\n",
    "#             torch.save(model.state_dict(),'inception_pm_train.pth')\n",
    "            best_loss = val_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
